\chapter{Thực nghiệm}
\section{Bộ dữ liệu}

Tiểu luận này sử dụng bộ dữ liệu gán nhãn từ loại tiếng Việt \texttt{pos\_tagging\_dataset} \cite{dataset}, được công bố trên nền tảng Kaggle. Toàn bộ bộ dữ liệu bao gồm 9511 câu đã được gán nhãn.

Dữ liệu thô tuân theo định dạng \texttt{từ/nhãn}, trong đó mỗi từ (token) và nhãn từ loại (POS tag) tương ứng được phân tách bởi ký tự gạch chéo (`/`). Một câu trong bộ dữ liệu có dạng như sau:

\begin{quote}
 % Sử dụng font nhỏ để câu ví dụ không bị tràn
\texttt{Trên/E đường/N xuất\_hiện/V nhiều/A cặp/N cha/N -/- con/N ,/, mẹ/N -/- con/N ,/, hay/C có\_khi/Ny là/C cả/T nhà/N ,/, tay\_xách\_nách\_mang/Ny vừa/R đi/V vừa/R dò/V bản\_đồ/N ./. }
\end{quote}

Sau quá trình tiền xử lý và tách nhãn, việc phân tích thống kê được thực hiện trên toàn bộ 9511 câu.

\subparagraph{Thống kê độ dài câu:}
Phân tích về số lượng token trong một câu cho thấy các đặc điểm sau:
\begin{itemize}
    \item \textbf{Độ dài trung bình (mean):} 22.87 token mỗi câu.
    \item \textbf{Độ dài tối thiểu (min):} 6 token.
    \item \textbf{Độ dài tối đa (max):} 129 token.
\end{itemize}

Phân bố chi tiết của độ dài câu được minh hoạ trong Hình \ref{fig:sentence_length_hist}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/sen_length_distribution.png}
    \caption{Phân bố độ dài câu trong bộ dữ liệu.}
    \label{fig:sentence_length_hist}
\end{figure}
% --------------------------------------------------

\subparagraph{Thống kê phân bố nhãn từ loại (POS Tag):}
Bộ dữ liệu bao gồm 20 nhãn từ loại riêng biệt. Bảng \ref{tab:pos_tag_stats} trình bày chi tiết tần suất xuất hiện và tỷ lệ phần trăm của từng nhãn trên toàn bộ tập dữ liệu (sau khi đã tách từ và nhãn).

\begin{table}[H]
    \centering
    \caption{Thống kê phân bố các nhãn từ loại (POS Tag) trong bộ dữ liệu.}
    \label{tab:pos_tag_stats}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lrr | lrr}
        \toprule
        \textbf{Nhãn (Tag)} & \textbf{Số lượng} & \textbf{Tỷ lệ (\%)} & \textbf{Nhãn (Tag)} & \textbf{Số lượng} & \textbf{Tỷ lệ (\%)} \\
        \midrule
        N     & 51557 & 23.70\% & L     & 3934  & 1.81\% \\
        V     & 42597 & 19.58\% & X     & 2187  & 1.01\% \\
        PUNCT & 32115 & 14.76\% & T     & 1407  & 0.65\% \\
        R     & 15822 & 7.27\%  & Nu    & 1032  & 0.47\% \\
        E     & 13541 & 6.22\%  & Nb    & 402   & 0.18\% \\
        A     & 13427 & 6.17\%  & S     & 161   & 0.07\% \\
        P     & 9265  & 4.26\%  & I     & 91    & 0.04\% \\
        Np    & 8518  & 3.92\%  & Y     & 53    & 0.02\% \\
        M     & 8085  & 3.72\%  & B     & 23    & 0.01\% \\
        C     & 7892  & 3.63\%  &       &       &        \\
        Nc    & 5448  & 2.50\%  &       &       &        \\
        \bottomrule
    \end{tabular}
    }
\end{table}

Phân tích từ Bảng \ref{tab:pos_tag_stats} cho thấy rõ \textbf{sự mất cân bằng} về phân phối của dữ liệu. Ba nhãn phổ biến nhất là \textbf{N} (Danh từ), \textbf{V} (Động từ) và \textbf{PUNCT} (Dấu câu) chiếm tới 58.04\% tổng số nhãn. Ngược lại, các nhãn như \textbf{I}, \textbf{Y}, và \textbf{B} là cực kỳ hiếm, mỗi nhãn chiếm dưới 0.05\%. Sự mất cân bằng này là một yếu tố quan trọng ảnh hưởng trực tiếp đến quá trình huấn luyện và đánh giá các mô hình, đặc biệt là đối với các lớp thiểu số.

Sau đó, quá trình trích xuất đặc trưng bằng PhoBERT được áp dụng tuần tự cho từng câu trong bộ dữ liệu. Do PhoBERT hoạt động ở cấp độ subword, một quy trình tổng hợp (mean pooling) đã được thực hiện để ánh xạ các vector subword trở lại 217.557 token gốc.

Kết quả của quy trình này là 217.557 bản ghi, trong đó mỗi bản ghi là một cặp (vector, nhãn). Phần vector là một biểu diễn ngữ nghĩa có 768 chiều, được trích xuất từ tầng cuối cùng của mô hình PhoBERT, thể hiện ngữ cảnh của token đó trong câu. Phần nhãn là POS tag tương ứng với token.

Như vậy, từ 9511 câu, một bộ dữ liệu dạng bảng đã được xây dựng, bao gồm 217.557 mẫu, với mỗi mẫu là một vector 768 chiều, đây là dữ liệu sẽ được dùng để huấn luyện các mô hình học máy.

\section{Phân tích tương quan và trực quan hóa dữ liệu}

Do vector đặc trưng từ PhoBERT có số chiều lớn ($d=768$), việc trực quan hóa trực tiếp là bất khả thi. Do đó, báo cáo áp dụng hai kỹ thuật giảm chiều đại diện cho hai hướng tiếp cận khác nhau:
\begin{enumerate}
    \item \textbf{Principal Component Analysis (PCA):} Phương pháp giảm chiều không giám sát, tập trung vào việc bảo toàn phương sai lớn nhất của dữ liệu, giúp quan sát cấu trúc phân phối tự nhiên.
    \item \textbf{Linear Discriminant Analysis (LDA):} Phương pháp giảm chiều có giám sát, tập trung vào việc tối đa hóa khoảng cách giữa các lớp, giúp đánh giá khả năng phân tách của dữ liệu.
\end{enumerate}

Dữ liệu được giảm xuống 6 chiều (6 thành phần chính) để thực hiện phân tích tương quan cặp (Pairwise Correlation Analysis).

\subsection{Phân tích cấu trúc dữ liệu với PCA}

Hình \ref{fig:pca_pair_plot} biểu diễn ma trận biểu đồ phân tán (Scatter matrix) và phân phối mật độ (Density plot) của 6 thành phần PCA đầu tiên.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/pca_6d.png} 
    \caption{Biểu đồ quan hệ cặp của 6 thành phần PCA.}
    \label{fig:pca_pair_plot}
\end{figure}

Từ biểu đồ PCA, ta có các quan sát quan trọng về tính chất thống kê của dữ liệu:

\begin{itemize}
    \item \textbf{Dạng phân phối Gaussian (Phân phối chuẩn):} 
    Quan sát các biểu đồ mật độ trên đường chéo chính (diagonal), ta thấy hầu hết các thành phần PCA đều tuân theo phân phối có dạng hình chuông (bell-curve) khá chuẩn, tập trung quanh giá trị trung bình là 0. Điều này xác nhận giả định rằng không gian vector của PhoBERT có tính chất phân phối chuẩn đa biến. Đây chính là cơ sở lý thuyết giải thích tại sao mô hình \textbf{Gaussian Naive Bayes} lại hoạt động hiệu quả (Accuracy tăng mạnh) sau khi áp dụng PCA, vì dữ liệu sau biến đổi đã thỏa mãn giả định của mô hình này.

    \item \textbf{Sự chồng lấn giữa các lớp:}
    Trên các biểu đồ phân tán, các điểm dữ liệu của các nhãn khác nhau (biểu thị bằng các màu sắc khác nhau) bị trộn lẫn thành một khối dày đặc (dense blob) ở trung tâm. Không có ranh giới rõ ràng nào được hình thành. Điều này phản ánh bản chất của PCA là \textit{không giám sát}: nó chỉ tìm hướng mà dữ liệu biến thiên mạnh nhất (variance) chứ không quan tâm đến việc tách biệt các nhãn. Điều này giải thích tại sao việc giảm chiều bằng PCA lại gây hại cho các mô hình có khả năng học phi tuyến mạnh như MLP hay Softmax Regression, do PCA đã vô tình loại bỏ các thông tin phân loại nằm ở các chiều có phương sai thấp.

    \item \textbf{Tính độc lập của các chiều:}
    Các trục của đám mây dữ liệu trong các biểu đồ cặp thường song song với các trục toạ độ, cho thấy các thành phần PCA đã khử tương quan tốt.
\end{itemize}

\subsection{Phân tích khả năng phân tách với LDA}

Khác với PCA, LDA sử dụng thông tin nhãn để tìm ra không gian chiếu tối ưu cho việc phân loại. Hình \ref{fig:lda_pair_plot} minh họa kết quả khi giảm xuống 6 chiều bằng LDA.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/lda_6d.png} 
    \caption{Biểu đồ quan hệ cặp của 6 thành phần LDA.}
    \label{fig:lda_pair_plot}
\end{figure}

Kết quả từ LDA cho thấy một bức tranh hoàn toàn khác biệt và giàu thông tin hơn về khả năng phân loại:

\begin{itemize}
    \item \textbf{Sự tách biệt tuyệt đối của lớp Dấu câu (PUNCT):} 
    Quan sát thành phần \textbf{LDA1} (trục hoành ở hàng 1 hoặc trục tung ở cột 1), ta thấy một cụm dữ liệu màu xanh lam (tương ứng với nhãn PUNCT) bị tách biệt hoàn toàn khỏi phần còn lại của dữ liệu. Trên biểu đồ mật độ của LDA1 (ô đầu tiên góc trái trên), có hai đỉnh phân phối riêng biệt. Điều này chứng tỏ rằng trong không gian ngữ nghĩa của PhoBERT, các dấu câu mang một đặc trưng thống kê cực kỳ khác biệt so với từ ngữ thông thường. LDA1 chính là "trục đặc trưng" chuyên biệt để nhận diện dấu câu.

    \item \textbf{Sự hình thành các cụm từ loại:}
    Khác với khối "blob" hỗn độn của PCA, các biểu đồ LDA (ví dụ: LDA2 vs LDA3, LDA4 vs LDA5) cho thấy sự hình thành của các cụm (clusters) rõ rệt. 
    \begin{itemize}
        \item Các điểm màu xanh lá cây nhạt (thường là Danh từ - N) có xu hướng tụ lại thành một vùng.
        \item Các điểm màu tím/hồng (thường là Động từ/Tính từ) tụ lại ở một vùng đối diện.
    \end{itemize}
    Sự phân tách này chứng minh rằng dữ liệu gốc thực sự chứa thông tin để phân loại, và LDA đã cô đọng những thông tin đó vào 6 chiều này.

    \item \textbf{Vùng chồng lấn:}
    Mặc dù tách cụm tốt hơn PCA, nhưng sự giao thoa giữa các lớp vẫn tồn tại (ví dụ: giữa cụm màu vàng và màu xanh lá trong LDA4). Đây chính là những trường hợp "nhập nhằng" trong ngôn ngữ (ví dụ: từ "cày" vừa có thể là Danh từ, vừa là Động từ) mà các mô hình học máy sẽ gặp khó khăn.
\end{itemize}

\textbf{Kết luận sơ bộ từ trực quan hóa:} 
PCA cho thấy dữ liệu tuân theo phân phối chuẩn (ủng hộ việc dùng Naive Bayes), trong khi LDA khẳng định dữ liệu có cấu trúc phân tách tốt (ủng hộ tiềm năng đạt độ chính xác cao của Softmax Regression và MLP). Sự khác biệt giữa Hình \ref{fig:pca_pair_plot} và Hình \ref{fig:lda_pair_plot} chính là minh chứng trực quan cho sự đánh đổi giữa việc "giữ lại thông tin tổng quát" (PCA) và "tối ưu hóa cho phân loại" (LDA).

\subsection{Trực quan hóa không gian dữ liệu trên 2 chiều}

Để có cái nhìn tổng quan trực quan nhất, dữ liệu được nén sâu hơn nữa xuống còn 2 chiều (2D). Mặc dù việc nén từ 768 chiều xuống 2 chiều gây mất mát lượng lớn thông tin, nó cho phép ta quan sát trực tiếp cấu trúc toàn cục của dữ liệu.

\subsubsection{So sánh phân bố thực tế (True Labels) trên PCA và LDA}

Hình \ref{fig:2d_pca_true} và Hình \ref{fig:2d_lda_true} so sánh kết quả chiếu dữ liệu lên mặt phẳng 2D sử dụng PCA và LDA, với màu sắc đại diện cho nhãn từ loại thực tế (Ground Truth).

\begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{images/pca_2d_true.png}
        \caption{Phân bố nhãn thực tế trên không gian 2D PCA.}
        \label{fig:2d_pca_true}
\end{figure}

\begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{images/lda_2d_true.png}
        \caption{Phân bố nhãn thực tế trên không gian 2D LDA.}
        \label{fig:2d_lda_true}
\end{figure}

\textbf{Nhận xét:}
\begin{itemize}
    \item \textbf{PCA (Hình \ref{fig:2d_pca_true}):} Dữ liệu xuất hiện dưới dạng một khối cầu lớn ("blob") với các nhãn trộn lẫn vào nhau. Sự chồng chéo này xác nhận rằng các phương sai lớn nhất (Principal Components) của PhoBERT không nhất thiết tương ứng với sự khác biệt về từ loại. Tuy nhiên, ta vẫn thấy một sự phân hóa nhẹ: các điểm màu xanh tím (PUNCT) có xu hướng dạt về góc dưới bên phải, tách nhẹ khỏi đám mây trung tâm.
    
    \item \textbf{LDA (Hình \ref{fig:2d_lda_true}):} Sự khác biệt là cực kỳ rõ rệt. LDA đã ép không gian vector co cụm lại để tối đa hóa khoảng cách giữa các lớp.
    \begin{itemize}
        \item Cụm bên phải (màu xanh tím): Tương ứng hoàn toàn với lớp \textbf{PUNCT} (Dấu câu). Đây là minh chứng mạnh mẽ nhất cho thấy dấu câu có đặc trưng ngữ cảnh hoàn toàn khác biệt so với từ vựng.
        \item Cụm bên trái (nhiều màu): Là tập hợp của các từ loại thực (Danh, Động, Tính từ...). Dù chồng lấn, nhưng ta thấy sự phân tầng rõ rệt theo trục dọc (LDA Component 2): Phía trên tập trung nhiều màu vàng/hồng (nhóm từ chức năng/hư từ), phía dưới tập trung màu xanh lá (Danh từ).
    \end{itemize}
\end{itemize}

\subsection{Phân tích Phân cụm K-Means}

Để khám phá cấu trúc tiềm ẩn trong dữ liệu PhoBERT, phương pháp phân cụm K-Means được áp dụng trên không gian đặc trưng gốc (768 chiều). Mục tiêu là xác định xem liệu các từ loại có tự nhiên hình thành các nhóm riêng biệt hay không, và nếu có, số lượng cụm tối ưu là bao nhiêu.
\subsubsection{Xác định số cụm tối ưu ($K$)}
Việc lựa chọn số cụm $K$ được thực hiện thông qua phương pháp Elbow (dựa trên Inertia) và chỉ số Silhouette Score.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/optimal_k_kmeans.png}
    \caption{Phân tích tìm K tối ưu: Elbow Method (trục trái - xanh) và Silhouette Score (trục phải - đỏ).}
    \label{fig:optimal_k}
\end{figure}

Từ Hình \ref{fig:optimal_k}, ta quan sát thấy:
\begin{itemize}
    \item \textbf{Elbow Method:} Đường cong Inertia giảm đều đặn và không xuất hiện điểm gập (elbow) rõ rệt. Điều này ám chỉ dữ liệu trong không gian gốc phân bố khá liên tục, không bị chia cắt thành các cụm hình cầu rời rạc.
    \item \textbf{Silhouette Score:} Đạt đỉnh cao nhất tại \textbf{$K=5$} (với giá trị khoảng 0.06). Mặc dù giá trị tuyệt đối thấp (do sự chồng lấn dữ liệu), đỉnh cục bộ này gợi ý rằng dữ liệu có xu hướng gom thành 5 nhóm ngữ nghĩa lớn.
    \item \textbf{Quyết định:} Chọn $K=5$ để thực hiện phân cụm dù cho số lượng nhãn POS thực tế là 20 nhãn. Việc $K$ tối ưu nhỏ hơn nhiều so với số nhãn thực tế cho thấy PhoBERT có xu hướng gom các từ loại chi tiết vào các nhóm cú pháp lớn hơn (ví dụ: gom hết Danh từ riêng, Danh từ chung vào một nhóm; hoặc gom các hư từ vào một nhóm).
\end{itemize}

\subsubsection{Trực quan hóa kết quả phân cụm (Clustering Results)}

Sau khi huấn luyện K-Means với $K=5$, nhãn dự đoán (Cluster ID) được gán cho từng điểm dữ liệu và trực quan hóa lại trên nền 2D PCA và LDA (Hình \ref{fig:kmeans_pca}, \ref{fig:kmeans_lda}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/pca_2d_kmeans.png}
    \caption{Kết quả phân cụm K-Means ($K=5$) trên nền PCA.}
    \label{fig:kmeans_pca}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/lda_2d_kmeans.png}
    \caption{Kết quả phân cụm K-Means ($K=5$) trên nền LDA.}
    \label{fig:kmeans_lda}
\end{figure}

\textbf{Phân tích kết quả phân cụm:}

\begin{itemize}
    \item \textbf{Trên không gian PCA (Hình \ref{fig:kmeans_pca}):} 
    K-Means chia cắt "đám mây" dữ liệu thành các vùng lãnh thổ riêng biệt. Các cụm (0, 1, 2, 3, 4) được phân chia khá đều.
    \begin{itemize}
        \item Đáng chú ý, cụm màu xanh da trời (Cluster 4) ở góc dưới bên phải trùng khớp vị trí với nhóm \textbf{PUNCT} trong biểu đồ nhãn thật (Hình \ref{fig:2d_pca_true}). Điều này khẳng định K-Means đã "tự học" và phát hiện ra nhóm dấu câu mà không cần giáo viên.
    \end{itemize}

    \item \textbf{Trên không gian LDA (Hình \ref{fig:kmeans_lda}):} 
    Sự tương đồng giữa kết quả phân cụm không giám sát (K-Means) và không gian phân lớp có giám sát (LDA) là minh chứng rõ nét nhất cho cấu trúc nội tại của dữ liệu. Quan sát Hình \ref{fig:kmeans_lda}, ta thấy hai vùng dữ liệu tách biệt hoàn toàn và K-Means đã gán nhãn chúng theo quy luật:

    \begin{itemize}
        \item \textbf{Sự lẫn lộn nhóm dấu câu:} 
        Vùng dữ liệu bên phải (tương ứng với lớp PUNCT trong nhãn thật) được K-Means bao phủ bởi sự pha trộn của \textbf{Cụm 0 (Màu đỏ)}, \textbf{Cụm 2 (Màu xanh ngọc)} và \textbf{Cụm 3 (Màu xanh dương)}. 

        \item \textbf{Sự phân tầng ngữ nghĩa trong nhóm Từ ngữ:} 
        Vùng dữ liệu lớn bên trái (tương ứng với các từ loại N, V, A...) cho thấy sự phân chia ranh giới rõ rệt theo trục dọc giữa hai cụm lớn:
        \begin{itemize}
            \item \textbf{Cụm 4 (Màu tím):} Chiếm lĩnh phần phía trên của khối dữ liệu. Đối chiếu với biểu đồ nhãn thật (Hình \ref{fig:2d_lda_true}), vùng này thường tập trung các Động từ (V) và Tính từ (A).
            \item \textbf{Cụm 1 (Màu vàng xanh):} Chiếm lĩnh phần phía dưới. Đối chiếu với nhãn thật, vùng này tương ứng mạnh mẽ với các Danh từ (N).
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{Kết luận về thực nghiệm phân tích dữ liệu:}
Các kết quả trực quan hóa và phân cụm đã chứng minh rằng:
\begin{enumerate}
    \item Dữ liệu PhoBERT chứa đựng cấu trúc ngữ nghĩa phong phú, trong đó các nhóm từ loại có sự phân tách nhất định về mặt hình học.
    \item \textbf{Dấu câu (Punctuation)} là lớp dễ phân biệt nhất, được tách biệt hoàn toàn bởi phương pháp không giám sát (K-Means) nhưng lại gây khó khăn cho phương pháp có giám sát (LDA).
    \item Sự chồng lấn lớn giữa các từ loại (Danh/Động/Tính) trong không gian 2D giải thích tại sao các mô hình đơn giản (như Naive Bayes trên dữ liệu gốc) lại gặp khó khăn, và cần đến các mô hình phi tuyến mạnh (như MLP) hoặc không gian chiều cao để phân loại chính xác.
\end{enumerate}

\section{Môi trường thực nghiệm}

Các thí nghiệm trong nghiên cứu này được triển khai và thực thi trên nền tảng Google Colab, một môi trường điện toán đám mây cho phép huấn luyện và đánh giá mô hình học máy trực tuyến. Việc sử dụng Google Colab giúp tận dụng hiệu quả tài nguyên tính toán, hỗ trợ cài đặt linh hoạt các thư viện Python chuyên dụng trong lĩnh vực xử lý ngôn ngữ tự nhiên.

Cấu hình phần cứng chính được sử dụng trong quá trình huấn luyện bao gồm GPU NVIDIA Tesla T4 với bộ nhớ đồ họa 15GB, cùng với môi trường phần mềm dựa trên Python 3.12.12 và các thư viện liên quan như \texttt{PyTorch}, \texttt{Transformers}, \texttt{Scikit-learn} và \texttt{NumPy}. Môi trường GPU giúp tăng tốc quá trình huấn luyện và suy luận của mô hình, đặc biệt khi sử dụng các mô hình ngôn ngữ lớn như PhoBERT.

Tất cả các thực nghiệm, bao gồm huấn luyện các mô hình, đều được thực hiện trong cùng điều kiện phần cứng và phần mềm nhằm đảm bảo tính nhất quán và khả năng tái lập của kết quả.

\section{Thực nghiệm}

Mục này trình bày chi tiết các kết quả thực nghiệm đạt được trên bộ dữ liệu POS Tagging tiếng Việt. Các thí nghiệm được thiết kế để đánh giá hiệu suất của mô hình trên hai bài toán riêng biệt nhưng có liên hệ mật thiết với nhau:
\begin{enumerate}
    \item \textbf{Bài toán Phân loại (Classification):} Gán nhãn từ loại cụ thể (N, V, A,...) cho từng token. Đây là bài toán chính của nghiên cứu.
    \item \textbf{Bài toán Hồi quy (Regression):} Dự đoán mức độ tin cậy (xác suất) của nhãn "Danh từ" (Noun) cho một token. Đây là bài toán mở rộng nhằm đánh giá khả năng mô hình hóa độ bất định ngữ nghĩa.
\end{enumerate}


% =================================================================
% PHẦN 1: KẾT QUẢ BÀI TOÁN PHÂN LOẠI
% =================================================================
\subsection{Kết quả Bài toán Phân loại}

Các thí nghiệm phân loại được đánh giá dựa trên các yếu tố biến thiên:
\begin{itemize}
    \item \textbf{Thuật toán:} Naive Bayes (NB), Softmax Regression (SR), và Multi-Layer Perceptron (MLP).
    \item \textbf{Số chiều đặc trưng:} Dữ liệu gốc (768 chiều) và dữ liệu giảm chiều bằng PCA (256, 64 chiều).
    \item \textbf{Phương pháp huấn luyện:} 
    \begin{itemize}
        \item \textit{Standard:} Huấn luyện bình thường, không can thiệp vào hàm mất mát.
        \item \textit{Weighted:} Huấn luyện có sử dụng phạt trọng số để xử lý mất cân bằng dữ liệu.
    \end{itemize}
    \item \textbf{Tỉ lệ Train/Test:} 80/20, 70/30, và 60/40.
\end{itemize}

Hai chỉ số đánh giá chính được sử dụng là Accuracy và F1-Score (cả Weighted và Macro).

\subsubsection{Kết quả định lượng}

Kết quả chi tiết được trình bày trong các Bảng \ref{tab:nb_results}, \ref{tab:sr_results}, và \ref{tab:mlp_results}. Các bảng này so sánh trực tiếp hiệu suất giữa phương pháp chuẩn (Standard) và phương pháp có trọng số (Weighted).

% ------- BẢNG 1: NAIVE BAYES (NB) -------
\begin{table}[H]
    \centering
    \caption{Kết quả so sánh mô hình Naive Bayes (GaussianNB).}
    \label{tab:nb_results}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{ll ccc | ccc | ccc} 
            \toprule
            \multirow{2}{*}{\textbf{Phương pháp}} & \multirow{2}{*}{\textbf{Số chiều}} & \multicolumn{3}{c}{\textbf{Accuracy}} & \multicolumn{3}{c}{\textbf{F1-Score (Weighted)}} & \multicolumn{3}{c}{\textbf{F1-Score (Macro)}} \\
            \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
             & & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} \\
            \midrule
            \multirow{3}{*}{\textbf{Standard}} 
             & 768 (Gốc) & 0.68 & 0.68 & 0.68 & 0.70 & 0.70 & 0.70 & 0.56 & 0.56 & 0.56 \\
             & 256       & \textbf{0.81} & \textbf{0.81} & \textbf{0.81} & \textbf{0.81} & \textbf{0.81} & \textbf{0.81} & \textbf{0.67} & \textbf{0.67} & \textbf{0.67} \\
             & 64        & 0.75 & 0.74 & 0.74 & 0.76 & 0.75 & 0.75 & 0.59 & 0.59 & 0.59 \\
            \midrule
            \multirow{3}{*}{\textbf{Weighted}} 
             & 768 (Gốc) & 0.67 & 0.67 & 0.67 & 0.69 & 0.69 & 0.69 & 0.54 & 0.54 & 0.55 \\
             & 256       & \textbf{0.79} & \textbf{0.79} & \textbf{0.79} & \textbf{0.80} & \textbf{0.80} & \textbf{0.80} & \textbf{0.63} & \textbf{0.63} & \textbf{0.64} \\
             & 64        & 0.69 & 0.68 & 0.68 & 0.72 & 0.71 & 0.71 & 0.52 & 0.51 & 0.51 \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

% ------- BẢNG 2: SOFTMAX REGRESSION (SR) -------
\begin{table}[H]
    \centering
    \caption{Kết quả so sánh mô hình Softmax Regression (SR).}
    \label{tab:sr_results}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{ll ccc | ccc | ccc}
            \toprule
            \multirow{2}{*}{\textbf{Phương pháp}} & \multirow{2}{*}{\textbf{Số chiều}} & \multicolumn{3}{c}{\textbf{Accuracy}} & \multicolumn{3}{c}{\textbf{F1-Score (Weighted)}} & \multicolumn{3}{c}{\textbf{F1-Score (Macro)}} \\
            \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
             & & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} \\
            \midrule
            \multirow{3}{*}{\textbf{Standard}}
             & 768 (Gốc) & \textbf{0.91} & \textbf{0.91} & \textbf{0.91} & \textbf{0.91} & \textbf{0.91} & \textbf{0.91} & \textbf{0.74} & \textbf{0.77} & \textbf{0.74} \\
             & 256       & 0.89 & 0.89 & \textbf{0.91} & 0.89 & 0.89 & \textbf{0.91} & 0.64 & 0.64 & \textbf{0.74} \\
             & 64        & 0.83 & 0.83 & 0.83 & 0.82 & 0.82 & 0.82 & 0.57 & 0.57 & 0.57 \\
            \midrule
            \multirow{3}{*}{\textbf{Weighted}}
             & 768 (Gốc) & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} & \textbf{0.91} & \textbf{0.76} & \textbf{0.74} & \textbf{0.75} \\
             & 256       & 0.88 & 0.88 & 0.88 & 0.88 & 0.88 & 0.88 & 0.68 & 0.68 & 0.68 \\
             & 64        & 0.80 & 0.79 & 0.79 & 0.81 & 0.81 & 0.81 & 0.58 & 0.58 & 0.58 \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

% ------- BẢNG 3: MULTI-LAYER PERCEPTRON (MLP) -------
\begin{table}[H]
    \centering
    \caption{Kết quả so sánh mô hình Multi-Layer Perceptron (MLP).}
    \label{tab:mlp_results}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{ll ccc | ccc | ccc}
        \toprule
        \multirow{2}{*}{\textbf{Phương pháp}} & \multirow{2}{*}{\textbf{Số chiều}} & \multicolumn{3}{c}{\textbf{Accuracy}} & \multicolumn{3}{c}{\textbf{F1-Score (Weighted)}} & \multicolumn{3}{c}{\textbf{F1-Score (Macro)}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
         & & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} & \textbf{80/20} & \textbf{70/30} & \textbf{60/40} \\
        \midrule
        \multirow{3}{*}{\textbf{Standard}}
         & 768 (Gốc) & \textbf{0.92} & \textbf{0.91} & \textbf{0.91} & \textbf{0.92} & \textbf{0.91} & \textbf{0.91} & \textbf{0.68} & \textbf{0.67} & \textbf{0.66} \\
         & 256       & 0.91 & \textbf{0.91} & \textbf{0.91} & 0.91 & \textbf{0.91} & \textbf{0.91} & 0.66 & 0.65 & 0.65 \\
         & 64        & 0.88 & 0.88 & 0.87 & 0.88 & 0.87 & 0.87 & 0.60 & 0.60 & 0.60 \\
        \midrule
        \multirow{3}{*}{\textbf{Weighted}}
         & 768 (Gốc) & \textbf{0.89} & \textbf{0.88} & \textbf{0.88} & \textbf{0.89} & \textbf{0.89} & \textbf{0.88} & \textbf{0.77} & \textbf{0.73} & \textbf{0.73} \\
         & 256       & 0.88 & 0.87 & 0.87 & 0.88 & 0.88 & \textbf{0.88} & 0.72 & 0.70 & 0.69 \\
         & 64        & 0.82 & 0.81 & 0.81 & 0.82 & 0.82 & 0.82 & 0.68 & 0.61 & 0.61 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

% ------- BẢNG 4: F1-SCORE CHI TIẾT (Thay đổi chú thích một chút để phù hợp) -------
\begin{table}[h!]
    \centering
    \small
    \setlength{\tabcolsep}{3pt} % Thu hẹp khoảng cách cột
    \caption{So sánh F1-Score chi tiết từng nhãn giữa phương pháp Standard (Std) và Weighted (Wgt).}
    \label{tab:f1_per_label_detailed}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lr | cc | cc | cc}
        \toprule
        \multirow{2}{*}{\textbf{Nhãn}} & \multirow{2}{*}{\textbf{Support}} & \multicolumn{2}{c|}{\textbf{Naive Bayes (256D)}} & \multicolumn{2}{c|}{\textbf{Softmax Reg (768D)}} & \multicolumn{2}{c}{\textbf{MLP (768D)}} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
         &  & \textbf{Std} & \textbf{Wgt} & \textbf{Std} & \textbf{Wgt} & \textbf{Std} & \textbf{Wgt} \\
        \midrule
        A     & 2685  & 0.71 & 0.69 & 0.79 & 0.79 & \textbf{0.82} & 0.78 \\
        B     & 4     & 0.22 & 0.12 & 0.00 & 0.80 & 0.00 & \textbf{0.86} \\
        C     & 1578  & 0.85 & 0.85 & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} & 0.91 \\
        E     & 2708  & 0.86 & 0.85 & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} & 0.92 \\
        I     & 18    & 0.44 & 0.44 & 0.50 & 0.50 & 0.00 & \textbf{0.61} \\
        L     & 787   & 0.92 & 0.92 & \textbf{0.97} & 0.95 & 0.96 & 0.95 \\
        M     & 1617  & 0.93 & 0.92 & \textbf{0.97} & \textbf{0.97} & \textbf{0.97} & \textbf{0.97} \\
        N     & 10308 & 0.81 & 0.78 & 0.91 & 0.90 & \textbf{0.92} & 0.88 \\
        Nb    & 80    & 0.45 & 0.32 & 0.55 & 0.57 & 0.49 & \textbf{0.59} \\
        Nc    & 1090  & 0.67 & 0.65 & 0.77 & 0.78 & \textbf{0.79} & 0.74 \\
        Np    & 1703  & 0.69 & 0.64 & 0.86 & 0.83 & \textbf{0.87} & 0.82 \\
        Nu    & 206   & 0.76 & 0.73 & 0.83 & \textbf{0.84} & \textbf{0.84} & 0.78 \\
        P     & 1853  & 0.85 & 0.85 & \textbf{0.94} & 0.92 & \textbf{0.94} & 0.92 \\
        PUNCT & 6421  & 0.90 & 0.90 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
        R     & 3164  & 0.81 & 0.79 & 0.89 & 0.87 & \textbf{0.90} & 0.87 \\
        S     & 32    & 0.46 & 0.34 & \textbf{0.83} & 0.40 & 0.00 & 0.67 \\
        T     & 281   & 0.46 & 0.38 & \textbf{0.58} & 0.53 & \textbf{0.58} & 0.54 \\
        V     & 8518  & 0.80 & 0.79 & 0.90 & 0.90 & \textbf{0.91} & 0.89 \\
        X     & 437   & 0.51 & 0.45 & \textbf{0.65} & 0.60 & \textbf{0.65} & 0.61 \\
        Y     & 11    & 0.22 & \textbf{0.23} & 0.00 & 0.20 & 0.00 & 0.13 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsubsection{Phân tích và đánh giá kết quả phân loại}

Dựa trên dữ liệu định lượng từ các Bảng \ref{tab:nb_results} đến \ref{tab:f1_per_label_detailed}, phần này trình bày các phân tích về hiệu năng mô hình, tác động của kỹ thuật giảm chiều dữ liệu và hiệu quả của chiến lược phạt trọng số (Class Weighting).

\paragraph{1. Hiệu suất tổng thể: } Đánh đổi giữa \textbf{độ chính xác} và \textbf{độ cân bằng}. 
Kết quả thực nghiệm trên toàn bộ tập dữ liệu cho thấy sự khác biệt về mục tiêu tối ưu hóa giữa các cấu hình mô hình:

\begin{itemize}
    \item \textbf{Về Độ chính xác (Accuracy):} Mô hình \textbf{Multi-Layer Perceptron (MLP) - Standard} trên không gian 768 chiều đạt kết quả cao nhất với Accuracy là \textbf{0.92} (tỉ lệ 80/20). Điều này chỉ ra rằng khi không áp dụng trọng số lớp, mạng nơ-ron sâu tận dụng hiệu quả khả năng học phi tuyến để tối ưu hóa dự đoán cho các mẫu dữ liệu thuộc các lớp phổ biến.
    
    \item \textbf{Về Độ cân bằng (Macro F1-Score):} Mô hình \textbf{MLP - Weighted} (768 chiều) đạt kết quả tốt nhất với F1-Macro là \textbf{0.77}. Mặc dù Accuracy tổng thể giảm nhẹ xuống 0.89 so với phiên bản Standard, chỉ số F1-Macro có sự cải thiện đáng kể (tăng từ 0.68 lên 0.77). Tương tự, Softmax Regression (Weighted) cũng duy trì tính ổn định cao với F1-Macro đạt 0.76.
    
    \item \textbf{Nhận xét:} Tồn tại sự đánh đổi (trade-off) giữa độ chính xác tổng thể và khả năng nhận diện các lớp thiểu số. Việc cải thiện độ cân bằng (Macro F1) thường đi kèm với mức giảm nhẹ khoảng 1-3\% về độ chính xác tổng thể (Accuracy).
\end{itemize}

\paragraph{2. Tác động của giảm chiều dữ liệu (PCA)}
Kết quả thực nghiệm khẳng định tác động trái chiều của phương pháp PCA đối với các loại mô hình khác nhau:
\begin{itemize}
    \item \textbf{Naive Bayes:} Hiệu suất thấp trên không gian gốc 768 chiều (Acc $\approx$ 0.68) nhưng được cải thiện rõ rệt khi giảm xuống 256 chiều (Acc $\approx$ 0.81). Nguyên nhân là do PCA giúp khử tương quan giữa các đặc trưng, thỏa mãn tốt hơn giả định độc lập của mô hình Gaussian Naive Bayes.
    \item \textbf{Softmax Regression \& MLP:} Đạt hiệu suất tối ưu trên không gian đặc trưng gốc (768 chiều) và suy giảm khi áp dụng PCA. Điều này cho thấy các mô hình này có khả năng tự trích xuất đặc trưng và xử lý nhiễu; việc nén dữ liệu đã làm mất đi một phần thông tin ngữ nghĩa chi tiết cần thiết để phân định các ranh giới quyết định phức tạp.
\end{itemize}

\paragraph{3. Phân tích hiệu quả của chiến lược Phạt trọng số (Weighted vs. Standard)}
Bảng \ref{tab:f1_per_label_detailed} cung cấp chi tiết về tác động của hàm mất mát có trọng số (Weighted Loss) đối với từng nhóm nhãn:

\subparagraph{a) Cải thiện hiệu năng trên các lớp thiểu số:}
Chiến lược Weighted cho thấy hiệu quả rõ rệt trong việc khắc phục tình trạng mô hình bỏ qua các lớp hiếm gặp.
\begin{itemize}
    \item \textbf{Nhãn B (4 mẫu):} Với phương pháp Standard, cả SR và MLP đều không nhận diện được (F1=0.00). Khi áp dụng Weighted, F1-Score của MLP tăng lên \textbf{0.86} và SR đạt 0.80.
    \item \textbf{Nhãn I (18 mẫu):} MLP Standard có F1=0.00, trong khi MLP Weighted đạt mức \textbf{0.61}.
    \item \textbf{Nhãn Nb (80 mẫu):} MLP Weighted cải thiện F1 từ 0.49 lên 0.59.
\end{itemize}
Việc gán trọng số phạt lớn cho các lớp thiểu số buộc thuật toán tối ưu hóa phải điều chỉnh ranh giới quyết định để bao quát các điểm dữ liệu này, thay vì coi chúng là nhiễu.

\subparagraph{b) Tác động đến các lớp đa số:}
Việc ưu tiên các lớp thiểu số dẫn đến sự suy giảm nhẹ về hiệu năng trên các lớp phổ biến.
\begin{itemize}
    \item Tại nhãn \textbf{N} (Danh từ), F1-Score của MLP giảm từ 0.92 (Standard) xuống 0.88 (Weighted).
    \item Tại nhãn \textbf{V} (Động từ), F1-Score giảm từ 0.91 xuống 0.89.
\end{itemize}
Đây là nguyên nhân chính dẫn đến việc Accuracy tổng thể của phiên bản Weighted thấp hơn so với Standard.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\textwidth]{images/confusion_matrix_softmax_weighted_768.png}
    \caption{Ma trận nhầm lẫn của phương pháp Hồi quy Softmax (Weighted) trên dữ liệu 768 chiều.}
    \label{fig:confusion_matrix_softmax_weighted_768}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\textwidth]{images/confusion_matrix_mlp_weighted_768.png}
    \caption{Ma trận nhầm lẫn của phương pháp Hồi quy MLP (Weighted) trên dữ liệu 768 chiều.}
    \label{fig:confusion_matrix_mlp_weighted_768}
\end{figure}


\subsection{Tối ưu hóa số chiều cho Naive Bayes}

Trong các thí nghiệm trước, ta đã thấy việc giảm chiều bằng PCA giúp cải thiện hiệu suất của Naive Bayes. Tuy nhiên, việc lựa chọn các mốc 256 hay 64 chiều mang tính chất cảm tính. Để khai thác tối đa tiềm năng của mô hình này, một thực nghiệm đã được thực hiện trên không gian số chiều từ 300 đến 500 nhằm tìm ra điểm mà mô hình đạt hiệu suất cao nhất.

Kết quả tìm kiếm trên ba tỉ lệ chia dữ liệu khác nhau được minh họa trong Hình \ref{fig:nb_pca_80_20}, \ref{fig:nb_pca_70_30} và \ref{fig:nb_pca_60_40}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/opt_dim_nb_82.png} 
    \caption{Tỉ lệ 80/20: Đỉnh tối ưu tại 430 chiều.}
    \label{fig:nb_pca_80_20}

\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/opt_dim_nb_73.png} 
    \caption{Tỉ lệ 70/30: Đỉnh tối ưu tại 450 chiều.}
    \label{fig:nb_pca_70_30}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/opt_dim_nb_64.png} 
    \caption{Tỉ lệ 60/40: Đỉnh tối ưu tại 490 chiều.}
    \label{fig:nb_pca_60_40}
\end{figure}

\textbf{Tổng hợp kết quả tối ưu:}

\begin{table}[H]
    \centering
    \caption{Số chiều PCA tối ưu cho Naive Bayes trên các tập dữ liệu khác nhau.}
    \label{tab:nb_optimal_dims}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Tỉ lệ Train/Test} & \textbf{Số chiều tối ưu} & \textbf{F1-Macro} & \textbf{F1-Weighted} \\
        \midrule
        80/20 & 430 & 0.6521 & $\approx$ 0.80 \\
        70/30 & 450 & 0.6495 & $\approx$ 0.80 \\
        60/40 & 490 & 0.6593 & $\approx$ 0.80 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Phân tích và nhận định:}

\begin{itemize}
    \item \textbf{Vùng tối ưu ổn định ở số chiều trung bình cao:}
    Thực nghiệm chỉ ra rằng Naive Bayes hoạt động hiệu quả nhất trong khoảng \textbf{430 đến 490 chiều} (tương đương 56\% - 64\% lượng thông tin gốc). Điều này bác bỏ giả định rằng giảm chiều sâu (xuống 64 hay 256) là tốt nhất. Kết quả này cho thấy để phân loại chính xác các từ loại trong tiếng Việt, mô hình cần giữ lại một lượng lớn phương sai để nắm bắt các ngữ cảnh phức tạp, trong khi việc loại bỏ khoảng 300 chiều cuối cùng (vốn chứa ít thông tin và nhiều nhiễu) giúp cải thiện tính tổng quát hóa.

    \item \textbf{Sự tách biệt giữa F1-Macro và Accuracy:}
    Quan sát biểu đồ, Accuracy và Weighted F1 (đường màu xanh) gần như đi ngang và rất ổn định. Sự biến động chủ yếu nằm ở chỉ số \textbf{F1-Macro}. Điều này khẳng định rằng việc tinh chỉnh số chiều PCA thực chất là quá trình tối ưu hóa khả năng nhận diện các \textit{lớp thiểu số}. Khi chọn đúng số chiều, mô hình "cứu" được các lớp hiếm khỏi bị nhiễu lấn át, trong khi các lớp phổ biến (N, V) đã đủ mạnh để không bị ảnh hưởng.

    \item \textbf{Mối quan hệ nghịch đảo giữa lượng dữ liệu và số chiều:}
    Một xu hướng thú vị được quan sát thấy: \textbf{Dữ liệu càng nhiều, số chiều tối ưu càng giảm}. 
    \begin{itemize}
        \item Khi có nhiều dữ liệu nhất (80/20), mô hình đạt đỉnh sớm nhất tại \textbf{430 chiều}.
        \item Khi dữ liệu ít hơn (60/40), mô hình cần tới \textbf{490 chiều} để đạt đỉnh.
    \end{itemize}
    Điều này mặc dù không rõ ràng nhưng có thể giải thích bởi: Khi tập huấn luyện đủ lớn (80\%), các thành phần chính đầu tiên ước lượng được phân phối của dữ liệu một cách chính xác và mạnh mẽ hơn, do đó mô hình có thể loại bỏ nhiễu ở các chiều cao sớm hơn. Ngược lại, khi dữ liệu ít (60\%), mô hình cần giữ lại thêm thông tin chi tiết ở các chiều cao hơn (từ 430 đến 490) để có đủ dữ kiện phân biệt các ranh giới mờ nhạt giữa các lớp.
\end{itemize}

\subsection{Kết quả Bài toán Hồi quy}

Mục tiêu của bài toán hồi quy là dự đoán độ tin cậy (xác suất) của nhãn "Danh từ" (N) dựa trên vector đặc trưng đầu vào. Để đảm bảo tính khách quan và tìm ra cấu hình mô hình tối ưu, nghiên cứu đã tiến hành thực nghiệm trên không gian tham số rộng, bao gồm:
\begin{itemize}
    \item \textbf{Tỉ lệ chia dữ liệu (Train/Test):} 80/20, 70/30, và 60/40.
    \item \textbf{Số chiều dữ liệu:} 768 (Gốc), 256 (PCA), 64 (PCA).
    \item \textbf{Tham số K (đối với KNN):} Chạy từ $K=1$ đến $K=9$.
\end{itemize}

\subsubsection{Khảo sát và Tối ưu hóa mô hình K-Nearest Neighbors (KNN)}

Do KNN là thuật toán lười (lazy learning) và nhạy cảm với cấu trúc dữ liệu cục bộ, việc tìm ra tham số $K$ và không gian đặc trưng phù hợp là bước tiên quyết.

\paragraph{1. Ảnh hưởng của Tỉ lệ chia dữ liệu và Số chiều:}
Bảng \ref{tab:knn_split_dim_summary} tổng hợp kết quả $R^2$ cao nhất (Peak $R^2$) đạt được tương ứng với mỗi cấu hình chia dữ liệu và số chiều.

\begin{table}[H]
    \centering
    \caption{Tổng hợp hiệu suất tối đa ($R^2$ Score) của KNN trên các kịch bản thực nghiệm khác nhau.}
    \label{tab:knn_split_dim_summary}
    \begin{tabular}{l | ccc}
        \toprule
        \multirow{2}{*}{\textbf{Tỉ lệ Train/Test}} & \multicolumn{3}{c}{\textbf{Hiệu suất tốt nhất ($R^2_{max}$) theo số chiều}} \\
        \cmidrule(lr){2-4}
         & \textbf{768 Chiều (Gốc)} & \textbf{256 Chiều (PCA)} & \textbf{64 Chiều (PCA)} \\
        \midrule
        \textbf{80/20} & \textbf{0.922} & \textbf{0.923} & \textbf{0.919} \\
        \textbf{70/30} & 0.917 & 0.918 & 0.915 \\
        \textbf{60/40} & 0.916 & 0.917 & 0.913 \\
        \bottomrule
    \end{tabular}
\end{table}

\textit{Nhận xét:}
\begin{itemize}
    \item \textbf{Tỉ lệ 80/20 mang lại kết quả tốt nhất:} Trên tất cả các số chiều, việc dành 80\% dữ liệu cho huấn luyện luôn cho kết quả $R^2$ cao nhất. Điều này dễ hiểu vì KNN cần mật độ dữ liệu tham chiếu dày đặc để đưa ra dự đoán chính xác cho các điểm mới.
    \item \textbf{Không gian 256 chiều (PCA) tối ưu nhẹ:} Hiệu suất trên 256 chiều ($R^2=0.923$) nhỉnh hơn một chút so với 768 chiều ($R^2=0.922$) và 64 chiều ($R^2=0.919$). PCA ở mức độ vừa phải (256D) giúp loại bỏ nhiễu dư thừa mà không làm mất thông tin quan trọng.
\end{itemize}

$\Rightarrow$ \textbf{Quyết định:} Chọn tỉ lệ chia \textbf{80/20} làm cơ sở để phân tích chi tiết tham số $K$ và so sánh với MLP.

\paragraph{2. Phân tích chi tiết tham số K (trên tập dữ liệu 80/20):}
Bảng \ref{tab:knn_k_detail} trình bày chi tiết sự biến thiên của MSE và $R^2$ khi thay đổi $K$ từ 1 đến 9 trên tập dữ liệu tốt nhất (80/20).

\begin{table}[H]
    \centering
    \caption{Chi tiết hiệu suất KNN theo tham số K (Tỉ lệ 80/20).}
    \label{tab:knn_k_detail}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c | cc | cc | cc}
        \toprule
        \multirow{2}{*}{\textbf{K}} & \multicolumn{2}{c|}{\textbf{768 Chiều (Gốc)}} & \multicolumn{2}{c|}{\textbf{256 Chiều (PCA)}} & \multicolumn{2}{c}{\textbf{64 Chiều (PCA)}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
         & \textbf{MSE} & \textbf{$R^2$} & \textbf{MSE} & \textbf{$R^2$} & \textbf{MSE} & \textbf{$R^2$} \\
        \midrule
        1 & 0.015 & 0.891 & 0.015 & 0.894 & 0.015 & 0.890 \\
        2 & 0.012 & 0.914 & 0.012 & 0.916 & 0.012 & 0.913 \\
        3 & 0.011 & 0.920 & 0.011 & 0.921 & 0.011 & 0.917 \\
        4 & 0.011 & 0.921 & 0.011 & 0.922 & \textbf{0.011} & \textbf{0.919} \\
        5 & \textbf{0.011} & \textbf{0.922} & \textbf{0.011} & \textbf{0.923} & 0.011 & 0.919 \\
        6 & 0.011 & 0.922 & 0.011 & 0.922 & 0.011 & 0.919 \\
        7 & 0.011 & 0.921 & 0.011 & 0.922 & 0.011 & 0.918 \\
        8 & 0.011 & 0.920 & 0.011 & 0.921 & 0.011 & 0.918 \\
        9 & 0.011 & 0.919 & 0.011 & 0.920 & 0.012 & 0.917 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\textit{Kết luận chọn tham số:} Dựa trên Bảng \ref{tab:knn_k_detail}, mô hình đạt đỉnh hiệu suất và ổn định nhất tại \textbf{K=5}. Các giá trị $K$ nhỏ hơn (1, 2) chịu ảnh hưởng lớn bởi nhiễu (Overfitting), trong khi $K$ lớn hơn (7, 8, 9) bắt đầu làm mờ ranh giới dự đoán (Underfitting).

\subsubsection{Hiệu suất của mô hình Multi-Layer Perceptron (MLP)}

Mô hình MLP được đánh giá trên cùng các kịch bản phân chia để đảm bảo tính công bằng. Kết quả thực nghiệm (Bảng \ref{tab:mlp_regression_results}) cho thấy MLP ít nhạy cảm với tỉ lệ chia dữ liệu hơn KNN nhưng lại rất nhạy cảm với số chiều đặc trưng.

\begin{table}[H]
    \centering
    \caption{Kết quả đánh giá mô hình MLP Regression trên các cấu hình khác nhau.}
    \label{tab:mlp_regression_results}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l | cc | cc | cc}
        \toprule
        \multirow{2}{*}{\textbf{Không gian đặc trưng}} & \multicolumn{2}{c|}{\textbf{Split 80/20}} & \multicolumn{2}{c|}{\textbf{Split 70/30}} & \multicolumn{2}{c}{\textbf{Split 60/40}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
         & \textbf{MSE} & \textbf{$R^2$} & \textbf{MSE} & \textbf{$R^2$} & \textbf{MSE} & \textbf{$R^2$} \\
        \midrule
        \textbf{768 Chiều (Gốc)} & \textbf{0.002} & \textbf{0.988} & \textbf{0.002} & \textbf{0.984} & \textbf{0.002} & \textbf{0.985} \\
        \textbf{256 Chiều (PCA)} & 0.004 & 0.972 & 0.004 & 0.969 & 0.004 & 0.969 \\
        \textbf{64 Chiều (PCA)}  & 0.009 & 0.936 & 0.009 & 0.933 & 0.010 & 0.931 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsubsection{Thảo luận và So sánh tổng hợp}

Dựa trên kết quả tối ưu của từng mô hình (KNN tại $K=5$, PCA-256D và MLP tại 768D), nghiên cứu rút ra các kết luận quan trọng sau:

\paragraph{1. Sự vượt trội của phương pháp Tham số hóa (MLP):}
So sánh cấu hình tốt nhất của hai mô hình trên cùng tỉ lệ chia 80/20:
\begin{itemize}
    \item \textbf{KNN (K=5, 256D):} $R^2 \approx 0.923$, MSE $\approx 0.011$.
    \item \textbf{MLP (768D):} $R^2 \approx 0.988$, MSE $\approx 0.002$.
\end{itemize}
Mô hình MLP giảm sai số MSE xuống hơn \textbf{5 lần} so với KNN. Kết quả $R^2$ tiệm cận 0.99 chứng tỏ mạng nơ-ron đã xấp xỉ gần như hoàn hảo hàm phân phối xác suất của Softmax Regression. Điều này khẳng định mối quan hệ giữa "vector ngữ nghĩa PhoBERT" và "độ tin cậy nhãn từ loại" là một mối quan hệ phi tuyến phức tạp mà các mô hình dựa trên khoảng cách đơn thuần như KNN không thể nắm bắt trọn vẹn.

\paragraph{2. Nghịch lý về chiều dữ liệu (Dimensionality Paradox):}
\begin{itemize}
    \item Với \textbf{KNN}, giảm chiều bằng PCA (xuống 256D) giúp cải thiện nhẹ hiệu suất ($R^2$ tăng từ 0.922 lên 0.923) do loại bỏ được nhiễu và tác động của lời nguyền số chiều.
    \item Với \textbf{MLP}, giảm chiều gây hại đáng kể ($R^2$ giảm từ 0.988 xuống 0.936). Mạng nơ-ron hoạt động như một bộ trích xuất đặc trưng tự động; việc nén dữ liệu tuyến tính bằng PCA trước đó đã làm mất đi các thông tin ngữ nghĩa tinh tế (nằm ở các chiều phương sai thấp) mà MLP cần để đạt độ chính xác cực đại.
\end{itemize}

\paragraph{Kết luận chung:} 
Việc chuyển đổi bài toán POS Tagging sang hồi quy là hoàn toàn khả thi. Mô hình \textbf{MLP trên không gian đặc trưng gốc (768 chiều)} là lựa chọn tối ưu nhất, mang lại độ chính xác gần như tuyệt đối và khả năng tổng quát hóa cao, mở ra tiềm năng ứng dụng trong việc gán nhãn mềm (soft-labeling) cho các tác vụ NLP tiếp theo.